# Arabic-Transformer
English to Arabic Transformer from scratch

An English → Arabic Transformer model implemented **from scratch** using PyTorch.  
This project focuses on understanding and implementing the core components of the Transformer architecture without relying on high-level libraries such as HuggingFace.

---

## Features
- Full Transformer architecture from scratch
- Multi-Head Self-Attention
- Positional Encoding
- Encoder–Decoder architecture
- Custom training loop
- GPU support (CUDA)
- Modular and extensible codebase

---

## Model Architecture
- Embedding dimension: 512
- Number of heads: 8
- Number of layers: 6
- Dropout: 0.1
- Scaled Dot-Product Attention
- Residual connections + LayerNorm

---

## Project Structure
